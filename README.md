# Text Generation using GPT-2

## Objective
To generate human-like text using a pre-trained transformer model.

## Model Used
GPT-2 (Transformer-based language model)

## How It Works
GPT-2 predicts the next word based on the entire context of the input sequence using self-attention mechanisms.

## Parameters Used
- max_length
- temperature
- top_k sampling
- top_p (nucleus sampling)

## Tools
- Python
- HuggingFace Transformers
- Google Colab

## Result
Successfully generated coherent and context-aware text using GPT-2.
